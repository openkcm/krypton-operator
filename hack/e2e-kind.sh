#!/bin/zsh
set -euo pipefail

OP_PID="" # will hold operator PID

HOME_CLUSTER=home
REMOTE_CLUSTER=remote
NAMESPACE=default
REMOTE_SECRET=demo-remote-1
REMOTE_SECRET_LABEL="sigs.k8s.io/multicluster-runtime-kubeconfig=true"
TENANT_NAME=e2e-echo
CHART_REPO=https://ealenn.github.io/charts
CHART_NAME=echo-server
CHART_VERSION=0.5.0
VALUES='{"replicaCount":1}'

log() { printf "[e2e] %s\n" "$*"; }

cleanup() {
  set +e
  if [ -n "$OP_PID" ] && kill -0 $OP_PID 2>/dev/null; then
    log "cleanup: sending TERM to operator PID=$OP_PID"
    kill $OP_PID 2>/dev/null || true
    # Wait up to 5s
    for i in {1..10}; do
      kill -0 $OP_PID 2>/dev/null || break
      sleep 0.5
    done
    if kill -0 $OP_PID 2>/dev/null; then
      log "cleanup: operator not exited; sending KILL"
      kill -9 $OP_PID 2>/dev/null || true
    fi
    else
    log "cleanup: operator already terminated or not started"
  fi
  log "tearing down kind clusters"
  kind delete cluster --name "$HOME_CLUSTER" 2>/dev/null || true
  kind delete cluster --name "$REMOTE_CLUSTER" 2>/dev/null || true
}
trap cleanup EXIT INT TERM

log "creating kind clusters"
kind create cluster --name "$HOME_CLUSTER"
# Small delay to reduce likelihood of port allocation race on macOS Docker.
sleep 3
kind create cluster --name "$REMOTE_CLUSTER"

log "wait for home cluster API health"
for i in {1..30}; do
  kubectl --context kind-$HOME_CLUSTER get --raw=/healthz >/dev/null 2>&1 && break
  sleep 2
  [ $i -eq 30 ] && { log "home cluster API not ready"; exit 1; }
done
log "home cluster API ready"

log "wait for remote cluster API health"
for i in {1..30}; do
  kubectl --context kind-$REMOTE_CLUSTER get --raw=/healthz >/dev/null 2>&1 && break
  sleep 2
  [ $i -eq 30 ] && { log "remote cluster API not ready"; exit 1; }
done
log "remote cluster API ready"

log "extract remote kubeconfig"
kind get kubeconfig --name "$REMOTE_CLUSTER" > /tmp/remote-kind.kubeconfig

log "extract home kubeconfig"
kind get kubeconfig --name "$HOME_CLUSTER" > /tmp/home-kind.kubeconfig

log "install Tenant CRD in home cluster (mesh.openkcm.io)"
kubectl --kubeconfig /tmp/home-kind.kubeconfig apply -f config/crd/bases/mesh.openkcm.io_tenants.yaml

log "install Tenant CRD in remote cluster (mesh.openkcm.io)"
kubectl --kubeconfig /tmp/remote-kind.kubeconfig apply -f config/crd/bases/mesh.openkcm.io_tenants.yaml

log "switch kubectl context to home cluster"
# IMPORTANT: KUBECONFIG must point to a file path, not contain the raw kubeconfig content.
# Previously we assigned the content which caused kubectl/client-go to treat segments of the
# kubeconfig YAML as file names, yielding "file name too long" errors while loading.
export KUBECONFIG=/tmp/home-kind.kubeconfig
if [ ! -s "$KUBECONFIG" ]; then
  log "home kubeconfig file missing or empty at $KUBECONFIG"
  exit 1
fi

log "create remote kubeconfig secret in home cluster"
kubectl -n "$NAMESPACE" create secret generic "$REMOTE_SECRET" --from-file=kubeconfig=/tmp/remote-kind.kubeconfig --dry-run=client -o yaml \
  | yq e '.metadata.labels."sigs.k8s.io/multicluster-runtime-kubeconfig"="true"' - \
  | kubectl apply -f -

log "build operator (cmd/krypton-operator)"
GO111MODULE=on go build -o /tmp/krypton-operator ./cmd/krypton-operator

log "apply RBAC (namespace delete permissions) to remote cluster"
kubectl --kubeconfig /tmp/remote-kind.kubeconfig apply -f config/rbac/role.yaml
kubectl --kubeconfig /tmp/remote-kind.kubeconfig apply -f config/rbac/rolebinding.yaml
# Grant namespace delete to the kubeconfig user used by the operator (out-of-cluster run)
USER_NAME=$(kubectl config view --kubeconfig /tmp/remote-kind.kubeconfig -o jsonpath='{.users[0].name}')
cat > /tmp/krypton-operator-e2e-user-binding.yaml << 'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: krypton-operator-e2e-user-role
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get","list","watch","create","update","patch","delete"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["get","list","watch","create","update","patch","delete"]
  - apiGroups: ["mesh.openkcm.io"]
    resources: ["tenants","tenants/status"]
    verbs: ["get","list","watch","create","update","patch","delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: krypton-operator-e2e-user-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: krypton-operator-e2e-user-role
subjects:
  - kind: User
    name: ${USER_NAME}
EOF
kubectl --kubeconfig /tmp/remote-kind.kubeconfig apply -f /tmp/krypton-operator-e2e-user-binding.yaml

log "start operator (background)"
# Require actual chart install (do not skip on non-fatal repo errors for this test).
unset ALLOW_CHART_SKIP || true
# Set Helm cache/config/data to writable temp locations for local runs
HELM_TMP_ROOT="/tmp/cryto-helm"
mkdir -p "$HELM_TMP_ROOT/cache/helm/repository" "$HELM_TMP_ROOT/config/helm" "$HELM_TMP_ROOT/data/helm"
export HELM_CACHE_HOME="$HELM_TMP_ROOT/cache/helm"
export HELM_CONFIG_HOME="$HELM_TMP_ROOT/config/helm"
export HELM_DATA_HOME="$HELM_TMP_ROOT/data/helm"
touch "$HELM_CONFIG_HOME/repositories.yaml"
(export HELM_REPOSITORY_CONFIG="$HELM_CONFIG_HOME/repositories.yaml"
 export HELM_REPOSITORY_CACHE="$HELM_CACHE_HOME/repository"
 export HELM_REGISTRY_CONFIG="$HELM_CONFIG_HOME/registry.json"
(KUBECONFIG=/tmp/home-kind.kubeconfig HELM_CACHE_HOME="$HELM_CACHE_HOME" HELM_CONFIG_HOME="$HELM_CONFIG_HOME" HELM_DATA_HOME="$HELM_DATA_HOME" /tmp/krypton-operator \
  -namespace "$NAMESPACE" \
  -kubeconfig-label sigs.k8s.io/multicluster-runtime-kubeconfig \
  -kubeconfig-key kubeconfig \
  -chart-repo "$CHART_REPO" \
  -chart-name "$CHART_NAME" \
  -chart-version "$CHART_VERSION") &
)
OP_PID=$!
echo $OP_PID > /tmp/krypton-operator.pid
sleep 6

log "apply tenant resource directly to remote cluster (no shadow model)"
kubectl --kubeconfig /tmp/remote-kind.kubeconfig apply -f - <<EOF
apiVersion: mesh.openkcm.io/v1alpha1
kind: Tenant
metadata:
  name: ${TENANT_NAME}
  namespace: ${NAMESPACE}
spec: {}
EOF

log "apply same tenant resource to home cluster for visibility"
kubectl --kubeconfig /tmp/home-kind.kubeconfig apply -f - <<EOF
apiVersion: mesh.openkcm.io/v1alpha1
kind: Tenant
metadata:
  name: ${TENANT_NAME}
  namespace: ${NAMESPACE}
spec: {}
EOF

log "check tenant phase on remote cluster (timeout 10m)"
# 10 minutes total: 200 iterations * 3s sleep = 600s.
for i in {1..200}; do
  PHASE=$(KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get tenant "$TENANT_NAME" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || true)
  [ "$PHASE" = "Ready" ] && break
  sleep 3
  [ $i -eq 200 ] && { log "tenant did not reach Ready phase within 10m"; KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get tenant "$TENANT_NAME" -n "$NAMESPACE" -o yaml; exit 1; }
done
log "tenant phase is Ready on remote cluster"

log "check tenant phase on home cluster (timeout 10m)"
for i in {1..200}; do
  PHASE_HOME=$(KUBECONFIG=/tmp/home-kind.kubeconfig kubectl get tenant "$TENANT_NAME" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || true)
  [ "$PHASE_HOME" = "Ready" ] && break
  sleep 3
  [ $i -eq 200 ] && { log "home cluster tenant did not reach Ready phase within 10m"; KUBECONFIG=/tmp/home-kind.kubeconfig kubectl get tenant "$TENANT_NAME" -n "$NAMESPACE" -o yaml; exit 1; }
done
log "tenant phase is Ready on home cluster"

log "verify deployments present in tenant namespace on remote cluster"
# Deployment names are prefixed by the Helm release name (tenant-<tenantName>-<secretName>) since we use the cluster Secret name.
RELEASE_PREFIX="tenant-${TENANT_NAME}-${REMOTE_SECRET}"
missingCount=0
for deploy in ${RELEASE_PREFIX}-echo-server; do
  for i in {1..60}; do
    OUT=$(KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get deploy "$deploy" -n "$TENANT_NAME" -o jsonpath='{.status.readyReplicas}' 2>/dev/null || true)
    if [ -n "$OUT" ] && [ "$OUT" -ge 1 ]; then
      log "deployment $deploy ready ($OUT replicas)"
      break
    fi
    sleep 2
    [ $i -eq 60 ] && { log "deployment $deploy not ready after 120s"; KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get deploy "$deploy" -n "$TENANT_NAME" -o yaml || { missingCount=$((missingCount+1)); }; break; }
  done
done

if [ $missingCount -gt 0 ]; then
  log "some expected deployments missing or not ready; listing all deployments for diagnostics"
  KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get deploy -n "$TENANT_NAME"
  log "checking for alternative prefix using cluster name"
  ALT_PREFIX="tenant-${TENANT_NAME}-${REMOTE_CLUSTER}"
  KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get deploy -n "$TENANT_NAME" | awk '{print $1}' | grep "${ALT_PREFIX}-echo-server" >/dev/null 2>&1 && {
    log "alternative prefix ${ALT_PREFIX} appears present; consider updating REMOTE_SECRET or release naming logic"
  }
  [ $missingCount -gt 0 ] && { log "e2e failing due to missing deployments"; exit 1; }
fi

log "verify echo-server pods running"
KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get pods -n "$TENANT_NAME"

# Delete tenant and verify cleanup (helm uninstall + namespace delete)
log "delete tenant on remote and home clusters"
kubectl --kubeconfig /tmp/remote-kind.kubeconfig delete tenant "$TENANT_NAME" -n "$NAMESPACE"
kubectl --kubeconfig /tmp/home-kind.kubeconfig delete tenant "$TENANT_NAME" -n "$NAMESPACE"

log "wait for namespace deletion on remote cluster"
for i in {1..120}; do
  KUBECONFIG=/tmp/remote-kind.kubeconfig kubectl get ns "$TENANT_NAME" >/dev/null 2>&1 || break
  sleep 2
  [ $i -eq 120 ] && { log "namespace $TENANT_NAME not deleted in remote cluster"; exit 1; }
done
log "tenant namespace deleted on remote cluster"

log "terminate operator (explicit)"
if [ -n "$OP_PID" ] && kill -0 $OP_PID 2>/dev/null; then
  log "operator still running, sending TERM signal"
  kill $OP_PID || true
  for i in {1..10}; do
    kill -0 $OP_PID 2>/dev/null || break
    sleep 0.3
  done
  if kill -0 $OP_PID 2>/dev/null; then
    log "force killing operator"
    kill -9 $OP_PID 2>/dev/null || true
  fi
else
  log "operator already terminated"
fi
# Clear PID so cleanup trap doesn't try to kill it again
OP_PID=""
rm -f /tmp/krypton-operator.pid

# Cleanup temp Helm dirs
rm -rf "$HELM_TMP_ROOT"

log "e2e passed"
